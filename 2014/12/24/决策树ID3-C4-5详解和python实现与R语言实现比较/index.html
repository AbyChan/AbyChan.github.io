
 <!DOCTYPE HTML>
<html lang="zh_CN">
<head>
  <meta charset="UTF-8">
  
    <title>决策树ID3;C4.5详解和python实现与R语言实现比较 | Tyan | AbyChan | Blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=3, minimum-scale=1">
    
    <meta name="author" content="Tyan">
    
    <meta name="description" content="把决策树研究一下，找来了一些自己觉得还可以的资料：
分类树（决策树）是一种十分常用的分类方法。他是一种监管学习，所谓监管学习说白了很简单，就是给定一堆样本，每个样本都有一组属性和一个类别，这些类别是事先确定的，那么通过学习得到一个分类器，这个分类器能够对新出现的对象给出正确的分类。这样的机器学习就被">
    
    
    
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/pacman.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/pacman.jpg">
    
    <link rel="stylesheet" href="/css/style.css" type="text/css">
</head>

  <body>
    <header>
      <div>
		
			<div id="imglogo">
				<a href="/"><img src="/img/logo.svg" alt="Tyan | AbyChan | Blog" title="Tyan | AbyChan | Blog"/></a>
			</div>
			
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="Tyan | AbyChan | Blog">Tyan | AbyChan | Blog</a></h1>
				<h2 class="blog-motto">沙漠的另一边 沙漠罢了</h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="菜單">
			</a></div>
			<nav class="animated">
				<ul>
					
						<li><a href="/">Home</a></li>
					
						<li><a href="/archives">Archives</a></li>
					
					<li>
					
					<form class="search" action="//google.com/search" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="text" id="search" name="q" autocomplete="off" maxlength="20" placeholder="搜索" />
						<input type="hidden" name="q" value="site:tyan.io">
					</form>
					
					</li>
				</ul>
			</nav>			
</div>

    </header>
    <div id="container">
      <div id="main" class="post" itemscope itemprop="blogPost">
	<article itemprop="articleBody"> 
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2014/12/24/决策树ID3-C4-5详解和python实现与R语言实现比较/" title="决策树ID3;C4.5详解和python实现与R语言实现比较" itemprop="url">决策树ID3;C4.5详解和python实现与R语言实现比较</a>
  </h1>
  <p class="article-author">By
    
      <a href="http://tyan.io" title="Tyan">Tyan</a>
    </p>
  <p class="article-time">
    <time datetime="2014-12-24T14:22:30.000Z" itemprop="datePublished">2014-12-24</time>
    更新日期:<time datetime="2015-07-17T02:58:34.284Z" itemprop="dateModified">2015-07-17</time>
    
  </p>
</header>
	<div class="article-content">
		
		
		<div id="toc" class="toc-article">
			<strong class="toc-title">文章目錄</strong>
		<ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-_信息论里的熵"><span class="toc-number">1.</span> <span class="toc-text">1. 信息论里的熵</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-_分类系统里的熵"><span class="toc-number">2.</span> <span class="toc-text">2. 分类系统里的熵</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-_信息增益和熵的关系"><span class="toc-number">3.</span> <span class="toc-text">3. 信息增益和熵的关系</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#一个例子："><span class="toc-number"></span> <span class="toc-text">一个例子：</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#用决策树来预测："><span class="toc-number"></span> <span class="toc-text">用决策树来预测：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#用熵来计算信息增益:"><span class="toc-number"></span> <span class="toc-text">用熵来计算信息增益:</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1_计算分类系统熵"><span class="toc-number">1.</span> <span class="toc-text">1 计算分类系统熵</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2_分别以Wind、Humidity、Outlook和Temperature作为根节点，计算其信息增益"><span class="toc-number">2.</span> <span class="toc-text">2 分别以Wind、Humidity、Outlook和Temperature作为根节点，计算其信息增益</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#C-45的算法"><span class="toc-number">3.</span> <span class="toc-text">C.45的算法</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#From_Garavan_Institute"><span class="toc-number"></span> <span class="toc-text">From Garavan Institute</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Documentation:_as_given_by_Ross_Quinlan"><span class="toc-number"></span> <span class="toc-text">Documentation: as given by Ross Quinlan</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6_databases_from_the_Garavan_Institute_in_Sydney,_Australia"><span class="toc-number"></span> <span class="toc-text">6 databases from the Garavan Institute in Sydney, Australia</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Approximately_the_following_for_each_database:"><span class="toc-number"></span> <span class="toc-text">Approximately the following for each database:</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#用R实现一下"><span class="toc-number"></span> <span class="toc-text">用R实现一下</span></a>
		</div>
		
		<p>把决策树研究一下，找来了一些自己觉得还可以的资料：</p>
<p>分类树（决策树）是一种十分常用的分类方法。他是一种监管学习，所谓监管学习说白了很简单，就是给定一堆样本，每个样本都有一组属性和一个类别，这些类别是事先确定的，那么通过学习得到一个分类器，这个分类器能够对新出现的对象给出正确的分类。这样的机器学习就被称之为监督学习。分类本质上就是一个map的过程。C4.5分类树就是决策树算法中最流行的一种。下面给出一个数据集作为算法例子的基础，比如有这么一个数据集，如下：</p>
<img src="/2014/12/24/决策树ID3-C4-5详解和python实现与R语言实现比较/20141225204507384.png" alt="20141225204507384.png" title="">
<p>这个Golf数据集就是我们这篇博客讨论的基础。我们分类的目的就是根据某一天的天气状态，如天气，温度，湿度，是否刮风，来判断这一天是否适合打高尔夫球。</p>
<img src="/2014/12/24/决策树ID3-C4-5详解和python实现与R语言实现比较/28142320-4394a37838e849678cb908301d8e7af5.jpg" alt="28142320-4394a37838e849678cb908301d8e7af5.jpg" title="">
<p>上面的图片已经很明显得说明了决策树的原理了</p>
<p>下面说说信息增益和熵</p>
<h4 id="1-_信息论里的熵">1. 信息论里的熵</h4><p>因此先回忆一下信息论中有关信息量（就是“熵”）的定义。说有这么一个变量X，它可能的取值有n多种，分别是x1，x2，……，xn，每一种取到的概率分别是P1，P2，……，Pn，那么X的熵就定义为：</p>
<img src="/2014/12/24/决策树ID3-C4-5详解和python实现与R语言实现比较/2012052716580125.jpg" alt="2012052716580125.jpg" title="">
<p>意思就是一个变量可能的变化越多（反而跟变量具体的取值没有任何关系，只和值的种类多少以及发生概率有关），它携带的信息量就越大（因此我一直觉得我们的政策法规信息量非常大，因为它变化很多，基本朝令夕改，笑）。</p>
<h4 id="2-_分类系统里的熵">2. 分类系统里的熵</h4><p>对分类系统来说，类别C是变量，它可能的取值是C1，C2，……，Cn，而每一个类别出现的概率是P(C1)，P(C2)，……，P(Cn)，因此n就是类别的总数。此时分类系统的熵就可以表示为：</p>
<img src="/2014/12/24/决策树ID3-C4-5详解和python实现与R语言实现比较/2012052716580941.jpg" alt="2012052716580941.jpg" title="">
<p>有同学说不好理解呀，这样想就好了，文本分类系统的作用就是输出一个表示文本属于哪个类别的值，而这个值可能是C1，C2，……，Cn，因此这个值所携带的信息量就是上式中的这么多。</p>
<h4 id="3-_信息增益和熵的关系">3. 信息增益和熵的关系</h4><p>信息增益是针对一个一个的特征而言的，就是看一个特征t，系统有它和没它的时候信息量各是多少，两者的差值就是这个特征给系统带来的信息量，即增益。系统含有特征t的时候信息量很好计算，就是刚才的式子，它表示的是包含所有特征时系统的信息量。</p>
<p>问题是当系统不包含t时，信息量如何计算？我们换个角度想问题，把系统要做的事情想象成这样：说教室里有很多座位，学生们每次上课进来的时 候可以随便坐，因而变化是很大的（无数种可能的座次情况）；但是现在有一个座位，看黑板很清楚，听老师讲也很清楚，于是校长的小舅子的姐姐的女儿托关系 （真辗转啊），把这个座位定下来了，每次只能给她坐，别人不行，此时情况怎样？对于座次的可能情况来说，我们很容易看出以下两种情况是等价的：（1）教室 里没有这个座位；（2）教室里虽然有这个座位，但其他人不能坐（因为反正它也不能参与到变化中来，它是不变的）。</p>
<p>对应到我们的系统中，就是下面的等价：（1）系统不包含特征t；（2）系统虽然包含特征t，但是t已经固定了，不能变化。</p>
<p>我们计算分类系统不包含特征t的时候，就使用情况（2）来代替，就是计算当一个特征t不能变化时，系统的信息量是多少。这个信息量其实也有专门的名称，就叫做“条件熵”，条件嘛，自然就是指“t已经固定“这个条件。</p>
<p>但是问题接踵而至，例如一个特征X，它可能的取值有n多种（x1，x2，……，xn）， 当计算条件熵而需要把它固定的时候，要把它固定在哪一个值上呢？答案是每一种可能都要固定一下，计算n个值，然后取均值才是条件熵。而取均值也不是简单的 加一加然后除以n，而是要用每个值出现的概率来算平均（简单理解，就是一个值出现的可能性比较大，固定在它上面时算出来的信息量占的比重就要多一些）。</p>
<p>因此有这样两个条件熵的表达式：</p>
<img src="/2014/12/24/决策树ID3-C4-5详解和python实现与R语言实现比较/2012052716582069.jpg" alt="2012052716582069.jpg" title="">
<p>这是指特征X被固定为值xi时的条件熵，</p>
<img src="/2014/12/24/决策树ID3-C4-5详解和python实现与R语言实现比较/2012052716582949.jpg" alt="2012052716582949.jpg" title="">
<p>这是指特征X被固定时的条件熵，注意与上式在意义上的区别。从刚才计算均值的讨论可以看出来，第二个式子与第一个式子的关系就是：</p>
<img src="/2014/12/24/决策树ID3-C4-5详解和python实现与R语言实现比较/2012052716583994.jpg" alt="2012052716583994.jpg" title="">
<p>具体到我们文本分类系统中的特征t，t有几个可能的值呢？注意t是指一个固定的特征，比如他就是指关键词“经济”或者“体育”，当我们说特征“经济”可能的取值时，实际上只有两个，“经济”要么出现，要么不出现。一般的，t的取值只有t（代表t出现）和clip_image006（代表t不出现），注意系统包含t但t 不出现与系统根本不包含t可是两回事。</p>
<p>因此固定t时系统的条件熵就有了，为了区别t出现时的符号与特征t本身的符号，我们用T代表特征，而用t代表T出现，那么：</p>
<img src="/2014/12/24/决策树ID3-C4-5详解和python实现与R语言实现比较/2012052716585043.jpg" alt="2012052716585043.jpg" title="">
<p>与刚才的式子对照一下，含义很清楚对吧，P(t)就是T出现的概率，就是T不出现的概率。这个式子可以进一步展开，其中的</p>
<img src="/2014/12/24/决策树ID3-C4-5详解和python实现与R语言实现比较/2012052716592052.jpg" alt="2012052716592052.jpg" title="">
<p>另一半就可以展开为：</p>
<img src="/2014/12/24/决策树ID3-C4-5详解和python实现与R语言实现比较/2012052716595056.jpg" alt="2012052716595056.jpg" title="">
<p>因此特征T给系统带来的信息增益就可以写成系统原本的熵与固定特征T后的条件熵之差：</p>
<img src="/2014/12/24/决策树ID3-C4-5详解和python实现与R语言实现比较/2012052716595874.jpg" alt="2012052716595874.jpg" title="">
<p>公式中的东西看上去很多，其实也都很好计算。比如P(Ci)，表示类别Ci出现的概率，其实只要用1除以类别总数就得到了（这是说你平等的看待每个类别而忽略它们的大小时这样算，如果考虑了大小就要把大小的影响加进去）。再比如P(t)，就是特征T出现的概率，只要用出现过T的文档数除以总文档数就可以了，再比如P(Ci|t)表示出现T的时候，类别Ci出现的概率，只要用出现了T并且属于类别Ci的文档数除以出现了T的文档数就可以了。</p>
<p>从以上讨论中可以看出，信息增益也是考虑了特征出现和不出现两种情况，与开方检验一样，是比较全面的，因而效果不错。但信息增益最大的问题 还在于它只能考察特征对整个系统的贡献，而不能具体到某个类别上，这就使得它只适合用来做所谓“全局”的特征选择（指所有的类都使用相同的特征集合），而 无法做“本地”的特征选择（每个类别有自己的特征集合，因为有的词，对这个类别很有区分度，对另一个类别则无足轻重）。</p>
<p>================</p>
<h1 id="一个例子：">一个例子：</h1><p>任务：</p>
<p>根据天气预测否去打网球</p>
<p>数据：</p>
<p>这个数据集来自Mitchell的机器学习，叫做是否去打网球play-tennis,以下数据仍然是从带逗号分割的文本文件，复制到纪事本，把后缀直接改为.csv就可以拿Excel打开：</p>
<blockquote>
<p><em>play-tennis data，其中6个变量依次为：编号、天气{Sunny、Overcast、Rain}、温度{热、冷、适中}、湿度{高、正常}、风力{强、弱}以及最后是否去玩的决策{是、否}。一个建议是把这些数据导入Excel后，另复制一份去掉变量的数据到另外一个工作簿，即只保留14个观测值。这样可以方便地使用Excel的排序功能，随时查看每个变量的取值到底有多少。</em>/</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">NO. , Outlook , Temperature , Humidity , Wind , Play&#10;1 , Sunny , Hot , High , Weak , No&#10;2 , Sunny , Hot , High , Strong , No&#10;3 , Overcast , Hot , High , Weak , Yes&#10;4 , Rain , Mild , High , Weak , Yes&#10;5 , Rain , Cool , Normal , Weak , Yes&#10;6 , Rain , Cool , Normal , Strong , No&#10;7 , Overcast , Cool , Normal , Strong , Yes&#10;8 , Sunny , Mild , High , Weak , No&#10;9 , Sunny , Cool , Normal , Weak , Yes&#10;10 , Rain , Mild , Normal , Weak , Yes&#10;11 , Sunny , Mild , Normal , Strong , Yes&#10;12 , Overcast , Mild , High , Strong , Yes&#10;13 , Overcast , Hot , Normal , Weak , Yes&#10;14 , Rain , Mild , High , Strong , No</span><br></pre></td></tr></table></figure>
<h3 id="用决策树来预测：">用决策树来预测：</h3><p>决策树的形式类似于“如果天气怎么样，去玩；否则，怎么着怎么着”的树形分叉。那么问题是用哪个属性（即变量，如天气、温度、湿度和风力）最适合充当这颗树的根节点，在它上面没有其他节点，其他的属性都是它的后续节点。</p>
<p>那么借用上面所述的能够衡量一个属性区分以上数据样本的能力的“信息增益”（Information Gain）理论。</p>
<p>如果一个属性的信息增益量越大，这个属性作为一棵树的根节点就能使这棵树更简洁，比如说一棵树可以这么读成，如果风力弱，就去玩；风力强，再按天气、温度等分情况讨论，此时用风力作为这棵树的根节点就很有价值。如果说，风力弱，再又天气晴朗，就去玩；如果风力强，再又怎么怎么分情况讨论，这棵树相比就不够简洁了。</p>
<h3 id="用熵来计算信息增益:">用熵来计算信息增益:</h3><h4 id="1_计算分类系统熵">1 计算分类系统熵</h4><p>类别是 是否出去玩。取值为yes的记录有9个，取值为no的有5个，即说这个样本里有9个正例，5 个负例，记为S(9+,5-)，S是样本的意思(Sample)。那么P(c1) = 9/14, P(c2) = 5/14</p>
<p>这里熵记为Entropy(S),计算公式为：</p>
<p>Entropy(S)= -(9/14)<em>log2(9/14)-(5/14)</em>log2(5/14)用Matlab做数学运算</p>
<h4 id="2_分别以Wind、Humidity、Outlook和Temperature作为根节点，计算其信息增益">2 分别以Wind、Humidity、Outlook和Temperature作为根节点，计算其信息增益</h4><p>我们来计算Wind的信息增益</p>
<p>当Wind固定为Weak时：记录有8条，其中正例6个，负例2个；</p>
<p>同样，取值为Strong的记录6个，正例负例个3个。我们可以计算相应的熵为：</p>
<p>Entropy(Weak)=-(6/8)<em>log(6/8)-(2/8)</em>log(2/8)=0.811<br>Entropy(Strong)=-(3/6)<em>log(3/6)-(3/6)</em>log(3/6)=1.0</p>
<p>现在就可以计算出相应的信息增益了：</p>
<p>所以，对于一个Wind属性固定的分类系统的信息量为 (8/14)<em>Entropy(Weak)+(6/14)</em>Entropy(Strong)</p>
<p>Gain(Wind)=Entropy(S)-(8/14)<em>Entropy(Weak)-(6/14)</em>Entropy(Strong)=0.940-(8/14)<em>0.811-(6/14)</em>1.0=0.048</p>
<p>这个公式的奥秘在于，8/14是属性Wind取值为Weak的个数占总记录的比例，同样6/14是其取值为Strong的记录个数与总记录数之比。</p>
<p>同理，如果以Humidity作为根节点：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Entropy(High)=0.985 ; Entropy(Normal)=0.592&#10;Gain(Humidity)=0.940-(7/14)*Entropy(High)-(7/14)*Entropy(Normal)=0.151&#10;&#20197;Outlook&#20316;&#20026;&#26681;&#33410;&#28857;&#65306;&#10;Entropy(Sunny)=0.971 ; Entropy(Overcast)=0.0 ; Entropy(Rain)=0.971&#10;Gain(Outlook)=0.940-(5/14)*Entropy(Sunny)-(4/14)*Entropy(Overcast)-(5/14)*Entropy(Rain)=0.247&#10;&#20197;Temperature&#20316;&#20026;&#26681;&#33410;&#28857;&#65306;&#10;Entropy(Cool)=0.811 ; Entropy(Hot)=1.0 ; Entropy(Mild)=0.918&#10;Gain(Temperature)=0.940-(4/14)*Entropy(Cool)-(4/14)*Entropy(Hot)-(6/14)*Entropy(Mild)=0.029</span><br></pre></td></tr></table></figure></p>
<p>这样我们就得到了以上四个属性相应的信息增益值：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Gain(Wind)=0.048 &#65307;Gain(Humidity)=0.151 &#65307; Gain(Outlook)=0.247 &#65307;Gain(Temperature)=0.029</span><br></pre></td></tr></table></figure></p>
<p>最后按照信息增益最大的原则选Outlook为根节点。子节点重复上面的步骤。这颗树可以是这样的，它读起来就跟你认为的那样：</p>
<img src="/2014/12/24/决策树ID3-C4-5详解和python实现与R语言实现比较/y2ps0b0BfKPn8s0Mib4zjkx_sg59UprpsZGlGKfhaHxNqYmkXgtuPafuow_efQQ94toLSocZ60vxYiBOVEH9ez6tA.jpeg" alt="y2ps0b0BfKPn8s0Mib4zjkx_sg59UprpsZGlGKfhaHxNqYmkXgtuPafuow_efQQ94toLSocZ60vxYiBOVEH9ez6tA.jpeg" title="">
<p>下面是《机器学习实战》的源码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line">Created on Oct <span class="number">12</span>, <span class="number">2010</span></span><br><span class="line">Decision Tree Source Code <span class="keyword">for</span> Machine Learning <span class="keyword">in</span> Action Ch. <span class="number">3</span></span><br><span class="line"><span class="decorator">@author: Peter Harrington</span></span><br><span class="line"><span class="string">'''</span><br><span class="line">from math import log</span><br><span class="line">import operator</span><br><span class="line"></span><br><span class="line">def createDataSet():</span><br><span class="line">    dataSet = [[1, 1, 'yes'],</span><br><span class="line">               [1, 1, 'yes'],</span><br><span class="line">               [1, 0, 'no'],</span><br><span class="line">               [0, 1, 'no'],</span><br><span class="line">               [0, 1, 'no']]</span><br><span class="line">    labels = ['no surfacing','flippers']</span><br><span class="line">    #change to discrete values</span><br><span class="line">    return dataSet, labels</span><br><span class="line"></span><br><span class="line">def calcShannonEnt(dataSet):</span><br><span class="line">    numEntries = len(dataSet)</span><br><span class="line">    labelCounts = &#123;&#125;</span><br><span class="line">    for featVec in dataSet: #the the number of unique elements and their occurance</span><br><span class="line">        currentLabel = featVec[-1]</span><br><span class="line">        if currentLabel not in labelCounts.keys(): labelCounts[currentLabel] = 0</span><br><span class="line">        labelCounts[currentLabel] += 1</span><br><span class="line">    shannonEnt = 0.0</span><br><span class="line">    for key in labelCounts:</span><br><span class="line">        prob = float(labelCounts[key])/numEntries</span><br><span class="line">        shannonEnt -= prob * log(prob,2) #log base 2</span><br><span class="line">    return shannonEnt</span><br><span class="line"></span><br><span class="line">def splitDataSet(dataSet, axis, value):</span><br><span class="line">    retDataSet = []</span><br><span class="line">    for featVec in dataSet:</span><br><span class="line">        if featVec[axis] == value:</span><br><span class="line">            reducedFeatVec = featVec[:axis]     #chop out axis used for splitting</span><br><span class="line">            reducedFeatVec.extend(featVec[axis+1:])</span><br><span class="line">            retDataSet.append(reducedFeatVec)</span><br><span class="line">    return retDataSet</span><br><span class="line"></span><br><span class="line">def chooseBestFeatureToSplit(dataSet):</span><br><span class="line">    numFeatures = len(dataSet[0]) - 1      #the last column is used for the labels</span><br><span class="line">    baseEntropy = calcShannonEnt(dataSet)</span><br><span class="line">    bestInfoGain = 0.0; bestFeature = -1</span><br><span class="line">    for i in range(numFeatures):        #iterate over all the features</span><br><span class="line">        featList = [example[i] for example in dataSet]#create a list of all the examples of this feature</span><br><span class="line">        uniqueVals = set(featList)       #get a set of unique values</span><br><span class="line">        newEntropy = 0.0</span><br><span class="line">        for value in uniqueVals:</span><br><span class="line">            subDataSet = splitDataSet(dataSet, i, value)</span><br><span class="line">            prob = len(subDataSet)/float(len(dataSet))</span><br><span class="line">            newEntropy += prob * calcShannonEnt(subDataSet)</span><br><span class="line">        infoGain = baseEntropy - newEntropy     #calculate the info gain; ie reduction in entropy</span><br><span class="line">        if (infoGain &gt; bestInfoGain):       #compare this to the best gain so far</span><br><span class="line">            bestInfoGain = infoGain         #if better than current best, set to best</span><br><span class="line">            bestFeature = i</span><br><span class="line">    return bestFeature                      #returns an integer</span><br><span class="line"></span><br><span class="line">def majorityCnt(classList):</span><br><span class="line">    classCount=&#123;&#125;</span><br><span class="line">    for vote in classList:</span><br><span class="line">        if vote not in classCount.keys(): classCount[vote] = 0</span><br><span class="line">        classCount[vote] += 1</span><br><span class="line">    sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True)</span><br><span class="line">    return sortedClassCount[0][0]</span><br><span class="line"></span><br><span class="line">def createTree(dataSet,labels):</span><br><span class="line">    classList = [example[-1] for example in dataSet]</span><br><span class="line">    if classList.count(classList[0]) == len(classList):</span><br><span class="line">        return classList[0]#stop splitting when all of the classes are equal</span><br><span class="line">    if len(dataSet[0]) == 1: #stop splitting when there are no more features in dataSet</span><br><span class="line">        return majorityCnt(classList)</span><br><span class="line">    bestFeat = chooseBestFeatureToSplit(dataSet)</span><br><span class="line">    bestFeatLabel = labels[bestFeat]</span><br><span class="line">    myTree = &#123;bestFeatLabel:&#123;&#125;&#125;</span><br><span class="line">    del(labels[bestFeat])</span><br><span class="line">    featValues = [example[bestFeat] for example in dataSet]</span><br><span class="line">    uniqueVals = set(featValues)</span><br><span class="line">    for value in uniqueVals:</span><br><span class="line">        subLabels = labels[:]       #copy all of labels, so trees don't mess up existing labels</span><br><span class="line">        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value),subLabels)</span><br><span class="line">    return myTree</span><br><span class="line"></span><br><span class="line">def classify(inputTree,featLabels,testVec):</span><br><span class="line">    firstStr = inputTree.keys()[0]</span><br><span class="line">    secondDict = inputTree[firstStr]</span><br><span class="line">    featIndex = featLabels.index(firstStr)</span><br><span class="line">    key = testVec[featIndex]</span><br><span class="line">    valueOfFeat = secondDict[key]</span><br><span class="line">    if isinstance(valueOfFeat, dict):</span><br><span class="line">        classLabel = classify(valueOfFeat, featLabels, testVec)</span><br><span class="line">    else: classLabel = valueOfFeat</span><br><span class="line">    return classLabel</span><br><span class="line"></span><br><span class="line">def storeTree(inputTree,filename):</span><br><span class="line">    import pickle</span><br><span class="line">    fw = open(filename,'w')</span><br><span class="line">    pickle.dump(inputTree,fw)</span><br><span class="line">    fw.close()</span><br><span class="line"></span><br><span class="line">def grabTree(filename):</span><br><span class="line">    import pickle</span><br><span class="line">    fr = open(filename)</span><br><span class="line">    return pickle.load(fr)</span></span><br></pre></td></tr></table></figure>
<p>生成图片的：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line">Created on Oct <span class="number">14</span>, <span class="number">2010</span></span><br><span class="line"></span><br><span class="line"><span class="decorator">@author: Peter Harrington</span></span><br><span class="line"><span class="string">'''</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">decisionNode = dict(boxstyle="sawtooth", fc="0.8")</span><br><span class="line">leafNode = dict(boxstyle="round4", fc="0.8")</span><br><span class="line">arrow_args = dict(arrowstyle="&lt;-")</span><br><span class="line"></span><br><span class="line">def getNumLeafs(myTree):</span><br><span class="line">    numLeafs = 0</span><br><span class="line">    firstStr = myTree.keys()[0]</span><br><span class="line">    secondDict = myTree[firstStr]</span><br><span class="line">    for key in secondDict.keys():</span><br><span class="line">        if type(secondDict[key]).__name__=='dict':#test to see if the nodes are dictonaires, if not they are leaf nodes</span><br><span class="line">            numLeafs += getNumLeafs(secondDict[key])</span><br><span class="line">        else:   numLeafs +=1</span><br><span class="line">    return numLeafs</span><br><span class="line"></span><br><span class="line">def getTreeDepth(myTree):</span><br><span class="line">    maxDepth = 0</span><br><span class="line">    firstStr = myTree.keys()[0]</span><br><span class="line">    secondDict = myTree[firstStr]</span><br><span class="line">    for key in secondDict.keys():</span><br><span class="line">        if type(secondDict[key]).__name__=='dict':#test to see if the nodes are dictonaires, if not they are leaf nodes</span><br><span class="line">            thisDepth = 1 + getTreeDepth(secondDict[key])</span><br><span class="line">        else:   thisDepth = 1</span><br><span class="line">        if thisDepth &gt; maxDepth: maxDepth = thisDepth</span><br><span class="line">    return maxDepth</span><br><span class="line"></span><br><span class="line">def plotNode(nodeTxt, centerPt, parentPt, nodeType):</span><br><span class="line">    createPlot.ax1.annotate(nodeTxt, xy=parentPt,  xycoords='axes fraction',</span><br><span class="line">             xytext=centerPt, textcoords='axes fraction',</span><br><span class="line">             va="center", ha="center", bbox=nodeType, arrowprops=arrow_args )</span><br><span class="line"></span><br><span class="line">def plotMidText(cntrPt, parentPt, txtString):</span><br><span class="line">    xMid = (parentPt[0]-cntrPt[0])/2.0 + cntrPt[0]</span><br><span class="line">    yMid = (parentPt[1]-cntrPt[1])/2.0 + cntrPt[1]</span><br><span class="line">    createPlot.ax1.text(xMid, yMid, txtString, va="center", ha="center", rotation=30)</span><br><span class="line"></span><br><span class="line">def plotTree(myTree, parentPt, nodeTxt):#if the first key tells you what feat was split on</span><br><span class="line">    numLeafs = getNumLeafs(myTree)  #this determines the x width of this tree</span><br><span class="line">    depth = getTreeDepth(myTree)</span><br><span class="line">    firstStr = myTree.keys()[0]     #the text label for this node should be this</span><br><span class="line">    cntrPt = (plotTree.xOff + (1.0 + float(numLeafs))/2.0/plotTree.totalW, plotTree.yOff)</span><br><span class="line">    plotMidText(cntrPt, parentPt, nodeTxt)</span><br><span class="line">    plotNode(firstStr, cntrPt, parentPt, decisionNode)</span><br><span class="line">    secondDict = myTree[firstStr]</span><br><span class="line">    plotTree.yOff = plotTree.yOff - 1.0/plotTree.totalD</span><br><span class="line">    for key in secondDict.keys():</span><br><span class="line">        if type(secondDict[key]).__name__=='dict':#test to see if the nodes are dictonaires, if not they are leaf nodes</span><br><span class="line">            plotTree(secondDict[key],cntrPt,str(key))        #recursion</span><br><span class="line">        else:   #it's a leaf node print the leaf node</span><br><span class="line">            plotTree.xOff = plotTree.xOff + 1.0/plotTree.totalW</span><br><span class="line">            plotNode(secondDict[key], (plotTree.xOff, plotTree.yOff), cntrPt, leafNode)</span><br><span class="line">            plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, str(key))</span><br><span class="line">    plotTree.yOff = plotTree.yOff + 1.0/plotTree.totalD</span><br><span class="line">#if you do get a dictonary you know it's a tree, and the first element will be another dict</span><br><span class="line"></span><br><span class="line">def createPlot(inTree):</span><br><span class="line">    fig = plt.figure(1, facecolor='white')</span><br><span class="line">    fig.clf()</span><br><span class="line">    axprops = dict(xticks=[], yticks=[])</span><br><span class="line">    createPlot.ax1 = plt.subplot(111, frameon=False, **axprops)    #no ticks</span><br><span class="line">    #createPlot.ax1 = plt.subplot(111, frameon=False) #ticks for demo puropses</span><br><span class="line">    plotTree.totalW = float(getNumLeafs(inTree))</span><br><span class="line">    plotTree.totalD = float(getTreeDepth(inTree))</span><br><span class="line">    plotTree.xOff = -0.5/plotTree.totalW; plotTree.yOff = 1.0;</span><br><span class="line">    plotTree(inTree, (0.5,1.0), '')</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">#def createPlot():</span><br><span class="line">#    fig = plt.figure(1, facecolor='white')</span><br><span class="line">#    fig.clf()</span><br><span class="line">#    createPlot.ax1 = plt.subplot(111, frameon=False) #ticks for demo puropses</span><br><span class="line">#    plotNode('a decision node', (0.5, 0.1), (0.1, 0.5), decisionNode)</span><br><span class="line">#    plotNode('a leaf node', (0.8, 0.1), (0.3, 0.8), leafNode)</span><br><span class="line">#    plt.show()</span><br><span class="line"></span><br><span class="line">def retrieveTree(i):</span><br><span class="line">    listOfTrees =[&#123;'no surfacing': &#123;0: 'no', 1: &#123;'flippers': &#123;0: 'no', 1: 'yes'&#125;&#125;&#125;&#125;,</span><br><span class="line">                  &#123;'no surfacing': &#123;0: 'no', 1: &#123;'flippers': &#123;0: &#123;'head': &#123;0: 'no', 1: 'yes'&#125;&#125;, 1: 'no'&#125;&#125;&#125;&#125;</span><br><span class="line">                  ]</span><br><span class="line">    return listOfTrees[i]</span><br><span class="line"></span><br><span class="line">#createPlot(thisTree)</span></span><br></pre></td></tr></table></figure></p>
<p>可以看出这个算法只能处理离散值，这就是ID3致命的缺点，而且ID3的信息增益会偏向value比较多的属性，于是C.45算法出现了</p>
<p>它是基于ID3算法进行改进后的一种重要算法，相比于ID3算法，改进有如下几个要点：</p>
<pre><code>用信息增益率来选择属性。ID3选择属性用的是子树的信息增益，这里可以用很多方法来定义信息，ID3使用的是熵（entropy， 熵是一种不纯度度量准则），也就是熵的变化值，而<span class="literal">C4</span>.<span class="number">5</span>用的是信息增益率。
在决策树构造过程中进行剪枝，因为某些具有很少元素的结点可能会使构造的决策树过适应（Overfitting），如果不考虑这些结点可能会更好。
对非离散数据也能处理。
能够对不完整数据进行处理。
</code></pre><p>首先，说明一下如何计算信息增益率。<br>熟悉了ID3算法后，已经知道如何计算信息增益，计算公式如下所示（来自Wikipedia）：</p>
<img src="/2014/12/24/决策树ID3-C4-5详解和python实现与R语言实现比较/info-gain.png" alt="info-gain.png" title="">
<p>或者，用另一个更加直观容易理解的公式计算：</p>
<ul>
<li>按照类标签对训练数据集D的属性集A进行划分，得到信息熵：</li>
</ul>
<img src="/2014/12/24/决策树ID3-C4-5详解和python实现与R语言实现比较/info.png" alt="info.png" title="">
<ul>
<li>按照属性集A中每个属性进行划分，得到一组信息：熵</li>
</ul>
<img src="/2014/12/24/决策树ID3-C4-5详解和python实现与R语言实现比较/infoA.png" alt="infoA.png" title="">
<ul>
<li>计算信息增益<br>然后计算信息增益，即前者对后者做差，得到属性集合A一组信息增益：</li>
</ul>
<img src="/2014/12/24/决策树ID3-C4-5详解和python实现与R语言实现比较/gain.png" alt="gain.png" title="">
<p>这样，信息增益就计算出来了。</p>
<ul>
<li>计算信息增益率<br>下面看，计算信息增益率的公式，如下所示（来自Wikipedia）：</li>
</ul>
<img src="/2014/12/24/决策树ID3-C4-5详解和python实现与R语言实现比较/IGR.png" alt="IGR.png" title="">
<p>其中，IG表示信息增益，按照前面我们描述的过程来计算。而IV是我们现在需要计算的，它是一个用来考虑分裂信息的度量，分裂信息用来衡量属性分 裂数据的广度和均匀程序，计算公式如下所示（来自Wikipedia）：</p>
<img src="/2014/12/24/决策树ID3-C4-5详解和python实现与R语言实现比较/IV.png" alt="IV.png" title="">
<p>简化一下，看下面这个公式更加直观：</p>
<img src="/2014/12/24/决策树ID3-C4-5详解和python实现与R语言实现比较/HV.png" alt="HV.png" title="">
<p>其中，V表示属性集合A中的一个属性的全部取值。</p>
<p>我们以一个很典型被引用过多次的训练数据集D为例，来说明C4.5算法如何计算信息增益并选择决策结点。</p>
<p>上面的训练集有4个属性，即属性集合A={OUTLOOK, TEMPERATURE, HUMIDITY, WINDY}；而类标签有2个，即类标签集合C={Yes, No}，分别表示适合户外运动和不适合户外运动，其实是一个二分类问题。<br>我们已经计算过信息增益，这里直接列出来，如下所示：<br>数据集D包含14个训练样本，其中属于类别“Yes”的有9个，属于类别“No”的有5个，则计算其信息熵：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Info(D) = -9/14 * log2(9/14) - 5/14 * log2(5/14) = 0.940</span><br></pre></td></tr></table></figure>
<p>下面对属性集中每个属性分别计算信息熵，如下所示：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Info(OUTLOOK) = 5/14 * [- 2/5 * log2(2/5) &#8211; 3/5 * log2(3/5)] + 4/14 * [ - 4/4 * log2(4/4) - 0/4 * log2(0/4)] + 5/14 * [ - 3/5 * log2(3/5) &#8211; 2/5 * log2(2/5)] = 0.694&#10;2   Info(TEMPERATURE) = 4/14 * [- 2/4 * log2(2/4) &#8211; 2/4 * log2(2/4)] + 6/14 * [ - 4/6 * log2(4/6) - 2/6 * log2(2/6)] + 4/14 * [ - 3/4 * log2(3/4) &#8211; 1/4 * log2(1/4)] = 0.911&#10;3   Info(HUMIDITY) = 7/14 * [- 3/7 * log2(3/7) &#8211; 4/7 * log2(4/7)] + 7/14 * [ - 6/7 * log2(6/7) - 1/7 * log2(1/7)] = 0.789&#10;4   Info(WINDY) = 6/14 * [- 3/6 * log2(3/6) &#8211; 3/6 * log2(3/6)] + 8/14 * [ - 6/8 * log2(6/8) - 2/8 * log2(2/8)] = 0.892</span><br></pre></td></tr></table></figure></p>
<p>根据上面的数据，我们可以计算选择第一个根结点所依赖的信息增益值，计算如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1   Gain(OUTLOOK) = Info(D) - Info(OUTLOOK) = 0.940 - 0.694 = 0.246&#10;2   Gain(TEMPERATURE) = Info(D) - Info(TEMPERATURE) = 0.940 - 0.911 = 0.029&#10;3   Gain(HUMIDITY) = Info(D) - Info(HUMIDITY) = 0.940 - 0.789 = 0.151&#10;4   Gain(WINDY) = Info(D) - Info(WINDY) = 0.940 - 0.892 = 0.048</span><br></pre></td></tr></table></figure>
<p>接下来，我们计算分裂信息度量H(V)：</p>
<ul>
<li><p>OUTLOOK属性<br>属性OUTLOOK有3个取值，其中Sunny有5个样本、Rainy有5个样本、Overcast有4个样本则，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1   H(OUTLOOK) = - 5/14 * log2(5/14) - 5/14 * log2(5/14) - 4/14 * log2(4/14) = 1.577406282852345</span><br></pre></td></tr></table></figure>
</li>
<li><p>TEMPERATURE属性<br>属性TEMPERATURE有3个取值，其中Hot有4个样本、Mild有6个样本、Cool有4个样本，则</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1   H(TEMPERATURE) = - 4/14 * log2(4/14) - 6/14 * log2(6/14) - 4/14 * log2(4/14) = 1.5566567074628228</span><br></pre></td></tr></table></figure>
</li>
<li><p>HUMIDITY属性<br>属性HUMIDITY有2个取值，其中Normal有7个样本、High有7个样本，则</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1   H(HUMIDITY) = - 7/14 * log2(7/14) - 7/14 * log2(7/14) = 1.0</span><br></pre></td></tr></table></figure>
</li>
<li><p>WINDY属性<br>属性WINDY有2个取值，其中True有6个样本、False有8个样本，则</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1   H(WINDY) = - 6/14 * log2(6/14) - 8/14 * log2(8/14) = 0.9852281360342516</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>根据上面计算结果，我们可以计算信息增益率，如下所示：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1   IGR(OUTLOOK) = Info(OUTLOOK) / H(OUTLOOK) = 0.246/1.577406282852345 = 0.15595221261270145&#10;2   IGR(TEMPERATURE) = Info(TEMPERATURE) / H(TEMPERATURE) = 0.029 / 1.5566567074628228 = 0.018629669509642094&#10;3   IGR(HUMIDITY) = Info(HUMIDITY) / H(HUMIDITY) = 0.151/1.0 = 0.151&#10;4   IGR(WINDY) = Info(WINDY) / H(WINDY) = 0.048/0.9852281360342516 = 0.048719680492692784</span><br></pre></td></tr></table></figure></p>
<p>根据计算得到的信息增益率进行选择属性集中的属性作为决策树结点，对该结点进行分裂。</p>
<ul>
<li>C4.5算法的优点是：产生的分类规则易于理解，准确率较高。</li>
<li>C4.5算法的缺点是：在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效。</li>
</ul>
<h4 id="C-45的算法">C.45的算法</h4><img src="/2014/12/24/决策树ID3-C4-5详解和python实现与R语言实现比较/201207252246294660.png" alt="201207252246294660.png" title="">
<p> 我自己用python实现了一下C.45<br>首先在UCI找数据<a href="http://archive.ics.uci.edu/ml/datasets.html" target="_blank" rel="external">http://archive.ics.uci.edu/ml/datasets.html</a><br>看中了这个<a href="http://archive.ics.uci.edu/ml/datasets/Thyroid+Disease" target="_blank" rel="external">http://archive.ics.uci.edu/ml/datasets/Thyroid+Disease</a></p>
<blockquote>
<p> Data Set Information:</p>
<h1 id="From_Garavan_Institute">From Garavan Institute</h1><h1 id="Documentation:_as_given_by_Ross_Quinlan">Documentation: as given by Ross Quinlan</h1><h1 id="6_databases_from_the_Garavan_Institute_in_Sydney,_Australia">6 databases from the Garavan Institute in Sydney, Australia</h1><h1 id="Approximately_the_following_for_each_database:">Approximately the following for each database:</h1></blockquote>
<p>嘿，还是C.45作者提供的数据呢，就选它了。<br>下了alldp.data和alldp.test</p>
<p>数据的文档头部需要自己添加header，把.|替换成,<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding=utf-8</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, val, child=[], condition=None)</span>:</span></span><br><span class="line">        self.val = val</span><br><span class="line">        self.child = child</span><br><span class="line">        self.condition = condition</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">C4_5</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#初始化</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, trainSet, format, rule)</span>:</span></span><br><span class="line">        self.tree = Node(<span class="keyword">None</span>, [])</span><br><span class="line">        trainSet = list(trainSet)</span><br><span class="line">        self.attributes = trainSet[<span class="number">0</span>][:-<span class="number">1</span>]</span><br><span class="line">        self.format = format</span><br><span class="line">        self.trainSet = trainSet[<span class="number">1</span>:]</span><br><span class="line">        self.dataLen = len(self.trainSet)</span><br><span class="line">        self.rule = rule</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">startTrain</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.train(self.trainSet, self.tree, self.attributes, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#处理缺失值，我这是图方便，实际测试predict时，不建议用测试样本中的数据来生成缺失数据，应该用训练数据来生成</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">rep_miss</span><span class="params">(self, dataSet)</span>:</span></span><br><span class="line">        exp = copy.deepcopy(dataSet)</span><br><span class="line">        <span class="keyword">for</span> attr <span class="keyword">in</span> self.attributes:</span><br><span class="line">            idx = self.attributes.index(attr)</span><br><span class="line">            <span class="keyword">if</span> self.format[idx] == <span class="string">'nominal'</span>:</span><br><span class="line">                <span class="comment">#expN 用频率最大的填补缺失</span></span><br><span class="line">                expN = getDefault([item[idx] <span class="keyword">for</span> item <span class="keyword">in</span> exp])</span><br><span class="line">                <span class="keyword">for</span> item <span class="keyword">in</span> exp:</span><br><span class="line">                    <span class="keyword">if</span> item[idx] == <span class="string">'?'</span>:</span><br><span class="line">                        item[idx] = expN</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    num_lst  = [float(item[idx]) <span class="keyword">for</span> item <span class="keyword">in</span> exp <span class="keyword">if</span> item[idx] != <span class="string">'?'</span>]</span><br><span class="line">                    mean = sum(num_lst) / len(num_lst)</span><br><span class="line">                    <span class="keyword">for</span> item <span class="keyword">in</span> exp:</span><br><span class="line">                        <span class="keyword">if</span> item[idx] == <span class="string">'?'</span>:</span><br><span class="line">                            item[idx] = mean</span><br><span class="line">        <span class="keyword">return</span> exp</span><br><span class="line"></span><br><span class="line">    <span class="comment">#寻找合适的分割点</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">split</span><span class="params">(self, lst, idx)</span>:</span></span><br><span class="line">        split_candidate = []</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> zip (lst, lst[<span class="number">1</span>:]):</span><br><span class="line">            <span class="keyword">if</span> (x[-<span class="number">1</span>] != y[-<span class="number">1</span>]) <span class="keyword">and</span> (x[idx] != y[idx]):</span><br><span class="line">                split_candidate.append( (x[idx] + y[idx]) / <span class="number">2</span> )</span><br><span class="line">        <span class="keyword">return</span> split_candidate</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">preProcess</span><span class="params">(self, validFile)</span>:</span></span><br><span class="line">        validSet = list( readData(validFile) )</span><br><span class="line">        validData = validSet[<span class="number">1</span>:]</span><br><span class="line">        exp = self.rep_miss(validData)</span><br><span class="line">        <span class="keyword">return</span> exp</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">rule_generator</span><span class="params">(self, tree, single_rule)</span>:</span></span><br><span class="line">        <span class="comment">#if flag:</span></span><br><span class="line">        <span class="comment">#   self.rule = []</span></span><br><span class="line">        <span class="comment">#print tree</span></span><br><span class="line">        <span class="keyword">if</span> tree.child:</span><br><span class="line">            <span class="keyword">if</span> isinstance(tree.val, list):</span><br><span class="line">                single_rule.append(tree.val)</span><br><span class="line">            <span class="keyword">if</span> tree.child[<span class="number">0</span>] == <span class="string">'negative'</span>:</span><br><span class="line">                single_rule.append([<span class="string">'class'</span>, <span class="string">'='</span>, <span class="string">'negative'</span>])</span><br><span class="line">                self.rule.append(single_rule)</span><br><span class="line">            <span class="keyword">elif</span> tree.child[<span class="number">0</span>] == <span class="string">'increased binding proteinval'</span>:</span><br><span class="line">                single_rule.append([<span class="string">'class'</span>, <span class="string">'='</span>, <span class="string">'increased binding proteinval'</span>])</span><br><span class="line">                self.rule.append(single_rule)</span><br><span class="line">            <span class="keyword">elif</span> tree.child[<span class="number">0</span>] == <span class="string">'decreased binding protein'</span>:</span><br><span class="line">                single_rule.append([<span class="string">'class'</span>, <span class="string">'='</span>, <span class="string">'decreased binding protein'</span>])</span><br><span class="line">                self.rule.append(single_rule)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">for</span> item <span class="keyword">in</span> tree.child:</span><br><span class="line">                    self.rule_generator(item, list(single_rule))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, dataSet, tree, attributes, default)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> len(dataSet) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> Node(default)</span><br><span class="line">        <span class="keyword">elif</span> allequal([item[-<span class="number">1</span>] <span class="keyword">for</span> item <span class="keyword">in</span> dataSet]):</span><br><span class="line">            <span class="keyword">return</span> Node(dataSet[<span class="number">0</span>][-<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">elif</span> len(attributes) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> Node(getDefault([item[-<span class="number">1</span>] <span class="keyword">for</span> item <span class="keyword">in</span> dataSet]))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment">#选取最大信息增益</span></span><br><span class="line">            best = self.choose_attr(attributes, dataSet)</span><br><span class="line">            <span class="keyword">if</span> best == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">return</span> Node(getDefault([item[-<span class="number">1</span>] <span class="keyword">for</span> item <span class="keyword">in</span> dataSet]))</span><br><span class="line">            <span class="keyword">print</span> best</span><br><span class="line">            tree.val = best[<span class="number">0</span>]</span><br><span class="line">            <span class="comment">#离散值的情况</span></span><br><span class="line">            idx = self.attributes.index(best[<span class="number">0</span>])</span><br><span class="line">            <span class="keyword">if</span> best[<span class="number">1</span>] == <span class="string">'nom'</span>:</span><br><span class="line">                attributes.remove(best[<span class="number">0</span>])</span><br><span class="line">                <span class="keyword">for</span> v <span class="keyword">in</span> unique(item[idx] <span class="keyword">for</span> item <span class="keyword">in</span> dataSet):</span><br><span class="line">                    subDataSet = [item <span class="keyword">for</span> item <span class="keyword">in</span> dataSet <span class="keyword">if</span> item[idx] == v]</span><br><span class="line">                    <span class="comment">#选取条件熵后的子数据集递归构造树</span></span><br><span class="line">                    subTree = self.train(subDataSet, Node(<span class="keyword">None</span>, []), list(attributes), getDefault(item[-<span class="number">1</span>] <span class="keyword">for</span> item <span class="keyword">in</span> dataSet))</span><br><span class="line">                    branch = Node([best[<span class="number">0</span>], <span class="string">'=='</span>, v], [subTree])</span><br><span class="line">                    tree.child.append(branch)</span><br><span class="line">            <span class="keyword">else</span>:<span class="comment">#连续型变量</span></span><br><span class="line">                subDataSet1 = [item <span class="keyword">for</span> item <span class="keyword">in</span> dataSet <span class="keyword">if</span> float(item[idx]) &gt; best[<span class="number">2</span>]]</span><br><span class="line">                default = getDefault(item[-<span class="number">1</span>] <span class="keyword">for</span> item <span class="keyword">in</span> dataSet)</span><br><span class="line">                <span class="keyword">if</span> len(subDataSet1) == len(dataSet):</span><br><span class="line">                    <span class="keyword">print</span> <span class="string">'!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!'</span></span><br><span class="line">                    <span class="keyword">return</span> default</span><br><span class="line">                subTree1 = self.train(subDataSet1, Node(<span class="keyword">None</span>), list(attributes), default)</span><br><span class="line">                subTree1.condition = [best[<span class="number">0</span>], <span class="string">'&gt;'</span>, str(best[<span class="number">2</span>])]</span><br><span class="line">                tree.child.append(subTree1)</span><br><span class="line"></span><br><span class="line">                subDataSet2 = [item <span class="keyword">for</span> item <span class="keyword">in</span> dataSet <span class="keyword">if</span> float(item[idx]) &lt;= best[<span class="number">2</span>]]</span><br><span class="line">                subTree2 = self.train(subDataSet2, Node(<span class="keyword">None</span>), list(attributes), default)</span><br><span class="line">                subTree2.condition=[best[<span class="number">0</span>], <span class="string">'&lt;='</span>, str(best[<span class="number">2</span>])]</span><br><span class="line">                tree.child.append(subTree2)</span><br><span class="line">            <span class="keyword">return</span> tree</span><br><span class="line"></span><br><span class="line">    <span class="comment">#求最大信息增益比</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_attr</span><span class="params">(self, attributes, dataSet)</span>:</span></span><br><span class="line">        maxIGR = <span class="number">0.0</span></span><br><span class="line">        dataLen = float(len(dataSet))</span><br><span class="line">        group = [item[-<span class="number">1</span>] <span class="keyword">for</span> item <span class="keyword">in</span> dataSet]</span><br><span class="line">        groupC = Counter(group).items()</span><br><span class="line">        <span class="comment">#sysGI 分类系统熵</span></span><br><span class="line">        sysGI = entropy([vl/dataLen <span class="keyword">for</span> k,vl <span class="keyword">in</span> groupC])</span><br><span class="line">        <span class="keyword">for</span> attr <span class="keyword">in</span> attributes:</span><br><span class="line">            idx = self.attributes.index(attr)</span><br><span class="line">            gain = sysGI</span><br><span class="line">            h = <span class="number">0.0</span> <span class="comment">#信息裂度</span></span><br><span class="line">            <span class="keyword">if</span> self.format[idx] == <span class="string">'nominal'</span>:</span><br><span class="line">                <span class="comment">#expN 把频率最大的填补缺失</span></span><br><span class="line">                expN = getDefault([item[idx] <span class="keyword">for</span> item <span class="keyword">in</span> dataSet])</span><br><span class="line">                <span class="keyword">for</span> item <span class="keyword">in</span> dataSet:</span><br><span class="line">                    <span class="keyword">if</span> item[idx] == <span class="string">'?'</span>:</span><br><span class="line">                        item[idx] = expN</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> unique([item[idx] <span class="keyword">for</span> item <span class="keyword">in</span> dataSet]):</span><br><span class="line">                    <span class="comment">#expG:该attr的所有分类结果</span></span><br><span class="line">                    expG = [item[-<span class="number">1</span>] <span class="keyword">for</span> item <span class="keyword">in</span> dataSet <span class="keyword">if</span> item[idx] == i]</span><br><span class="line">                    expGC = Counter(expG).items()</span><br><span class="line">                    split_len = float(len(expG))</span><br><span class="line">                    gain -= split_len/dataLen * entropy([vl/split_len <span class="keyword">for</span> k,vl <span class="keyword">in</span> expGC])</span><br><span class="line">                <span class="comment">#计算信息裂度</span></span><br><span class="line">                groupValueC = Counter([item[idx] <span class="keyword">for</span> item <span class="keyword">in</span> dataSet ]).items()</span><br><span class="line">                h -=  entropy([vl/len(dataSet) <span class="keyword">for</span> k,vl <span class="keyword">in</span> groupValueC])</span><br><span class="line">                <span class="keyword">if</span> h == <span class="number">0</span>:</span><br><span class="line">                    <span class="keyword">continue</span> <span class="comment">#不知道为什么会有0，郁闷</span></span><br><span class="line">                igr = gain / h</span><br><span class="line">                <span class="keyword">if</span> igr &gt; maxIGR:</span><br><span class="line">                    maxIGR = gain</span><br><span class="line">                    best = [attr, <span class="string">'nom'</span>]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                num_lst = [float(item[idx]) <span class="keyword">for</span> item <span class="keyword">in</span> dataSet <span class="keyword">if</span> item[idx] != <span class="string">'?'</span>]</span><br><span class="line">                <span class="keyword">if</span> len(num_lst) == <span class="number">0</span>:</span><br><span class="line">                    <span class="keyword">print</span> <span class="string">"Error!!!!"</span></span><br><span class="line">                mean = sum(num_lst) / len(num_lst)</span><br><span class="line">                exps = list(dataSet)</span><br><span class="line">                <span class="keyword">for</span> item <span class="keyword">in</span> exps:</span><br><span class="line">                    <span class="keyword">if</span> item[idx] == <span class="string">'?'</span>:</span><br><span class="line">                        item[idx] = mean</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        item[idx] = float(item[idx])</span><br><span class="line">                exps.sort(key = operator.itemgetter(idx))</span><br><span class="line">                split_candidate = self.split(exps, idx)</span><br><span class="line">                <span class="keyword">for</span> thresh <span class="keyword">in</span> split_candidate:</span><br><span class="line">                    gain = sysGI</span><br><span class="line">                     <span class="comment">#expG:该attr的所有分类结果</span></span><br><span class="line">                    expG1 = [item[-<span class="number">1</span>] <span class="keyword">for</span> item <span class="keyword">in</span> exps <span class="keyword">if</span> float(item[idx]) &gt; thresh]</span><br><span class="line">                    expG2 = [item[-<span class="number">1</span>] <span class="keyword">for</span> item <span class="keyword">in</span> exps <span class="keyword">if</span> float(item[idx]) &lt;= thresh]</span><br><span class="line">                    len1 = float(len(expG1))</span><br><span class="line">                    len2 = float(len(expG2))</span><br><span class="line">                    <span class="keyword">if</span> len1 == <span class="number">0</span> <span class="keyword">or</span> len2 == <span class="number">0</span>:</span><br><span class="line">                        gain = <span class="number">0</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        expGC1 = Counter(expG1).items()</span><br><span class="line">                        expGC2 = Counter(expG2).items()</span><br><span class="line">                        gain -= len1/dataLen * entropy([vl/len1 <span class="keyword">for</span> k,vl <span class="keyword">in</span> expGC1])</span><br><span class="line">                        gain -= len1/dataLen * entropy([vl/len1 <span class="keyword">for</span> k,vl <span class="keyword">in</span> expGC1])</span><br><span class="line">                    h -= entropy([len1/len(dataSet), len2/len(dataSet)])</span><br><span class="line">                    igr = gain / h</span><br><span class="line">                    <span class="keyword">if</span> igr &gt; maxIGR:</span><br><span class="line">                        maxIGR = igr</span><br><span class="line">                        best = [attr, <span class="string">'num'</span>, thresh]</span><br><span class="line">        <span class="comment">#print max_gain</span></span><br><span class="line">        <span class="keyword">if</span> maxIGR &lt;= <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> best</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">entropy</span><span class="params">(lst)</span>:</span></span><br><span class="line">    entrop = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> lst:</span><br><span class="line">        <span class="keyword">if</span> p == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        entrop -= p * math.log(p, <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> entrop</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">unique</span><span class="params">(seq)</span>:</span></span><br><span class="line">    keys = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> e <span class="keyword">in</span> seq:</span><br><span class="line">        keys[e] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> keys.keys()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">allequal</span><span class="params">(seq)</span>:</span></span><br><span class="line">    flag = seq[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> seq:</span><br><span class="line">        <span class="keyword">if</span> item != flag:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readData</span><span class="params">(inputfile)</span>:</span></span><br><span class="line">    data = []</span><br><span class="line">    abspath = os.path.abspath(inputfile)</span><br><span class="line">    <span class="keyword">with</span> open(abspath,<span class="string">"r"</span>)<span class="keyword">as</span> file:</span><br><span class="line">        text = file.readlines()</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> text:</span><br><span class="line">        items = line.split(<span class="string">','</span>)[:-<span class="number">1</span>]</span><br><span class="line">        items.pop(<span class="number">26</span>)</span><br><span class="line">        items.pop(<span class="number">26</span>)</span><br><span class="line">        data.append(items)</span><br><span class="line">    <span class="keyword">print</span> data[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">print</span> len(data[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line"><span class="comment">#这个函数是选取频率最大的</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getDefault</span><span class="params">(lst)</span>:</span></span><br><span class="line">    frequent = Counter(lst)</span><br><span class="line">    mostfrequent = frequent.most_common(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">if</span> mostfrequent[<span class="number">0</span>][<span class="number">0</span>] == <span class="string">'?'</span>:</span><br><span class="line">        mostfrequent = mostfrequent[<span class="number">1</span>:]</span><br><span class="line">    <span class="keyword">return</span> mostfrequent[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">format = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">28</span>):</span><br><span class="line">    format.append(<span class="string">"nominal"</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> [<span class="number">0</span>,<span class="number">17</span>,<span class="number">19</span>,<span class="number">21</span>,<span class="number">23</span>,<span class="number">25</span>]:</span><br><span class="line">    format[i] = <span class="string">"numeric"</span></span><br><span class="line"></span><br><span class="line">inputfile = <span class="string">"allbp"</span></span><br><span class="line">trainSet = readData(inputfile)</span><br><span class="line">classifier = C4_5(trainSet, format, [])</span><br><span class="line">classifier.startTrain()</span><br></pre></td></tr></table></figure></p>
<p>跑了一下，明显过拟合了(“▔□▔)/(“▔□▔)/<br>剪枝和测试没写(其实我平时用java，现在学python学得不好，写得烂，没有动力写下去了，惭愧啊 (╯#-_-)╯)</p>
<p>好吧，说说剪枝</p>
<p>过拟合是决策树的大问题</p>
<p>决策树为什么要剪枝？原因就是避免决策树“过拟合”样本。前面的算法生成的决策树非常的详细而庞大，每个属性都被详细地加以考虑，决策树的树叶节点所覆盖的训练样本都是“纯”的。因此用这个决策树来对训练样本进行分类的话，你会发现对于训练样本而言，这个树表现堪称完美，它可以100%完美正确得对训练样本集中的样本进行分类（因为决策树本身就是100%完美拟合训练样本的产物）。但是，这会带来一个问题，如果训练样本中包含了一些错误，按照前面的算法，这些错误也会100%一点不留得被决策树学习了，这就是“过拟合”。C4.5的缔造者昆兰教授很早就发现了这个问题，他作过一个试验，在某一个数据集中，过拟合的决策树的错误率比一个经过简化了的决策树的错误率要高。那么现在的问题就来了，如何在原生的过拟合决策树的基础上，通过剪枝生成一个简化了的决策树？</p>
<p>第一种方法，也是最简单的方法，称之为基于误判的剪枝。这个思路很直接，完全的决策树不是过度拟合么，我再搞一个测试数据集来纠正它。对于完全决策树中的每一个非叶子节点的子树，我们尝试着把它替换成一个叶子节点，该叶子节点的类别我们用子树所覆盖训练样本中存在最多的那个类来代替，这样就产生了一个简化决策树，然后比较这两个决策树在测试数据集中的表现，如果简化决策树在测试数据集中的错误比较少，并且该子树里面没有包含另外一个具有类似特性的子树（所谓类似的特性，指的就是把子树替换成叶子节点后，其测试数据集误判率降低的特性），那么该子树就可以替换成叶子节点。该算法以bottom-up的方式遍历所有的子树，直至没有任何子树可以替换使得测试数据集的表现得以改进时，算法就可以终止。</p>
<h1 id="用R实现一下">用R实现一下</h1><p>是c.45<br>读入数据，简单：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">thyData = read.table(<span class="string">"/home/tyan/DataSet/thyroid-disease/allbp"</span>,sep=<span class="string">','</span>,header=<span class="literal">T</span>,na.strings=<span class="string">'?'</span>)</span><br><span class="line">testData = read.table(<span class="string">"/home/tyan/DataSet/thyroid-disease/allbp.test"</span>,sep=<span class="string">','</span>,header=<span class="literal">T</span>,na.strings=<span class="string">'?'</span>)</span><br></pre></td></tr></table></figure>
<p>如果读了之后加header的话：<br><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">举个例子names(x) &lt;- c(<span class="string">'v1'</span>, <span class="string">'v2'</span>)</span><br></pre></td></tr></table></figure></p>
<p>加载一个lib之后一条语句模型就出来了<br><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">library</span>(rpart)</span><br><span class="line">rt&lt;-rpart(thyData$classes~.,data=thyData[,<span class="number">1</span>:<span class="number">26</span>])</span><br><span class="line">rt</span><br></pre></td></tr></table></figure></p>
<p>简单得吓人：<br><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">n= <span class="number">2800</span></span><br><span class="line"></span><br><span class="line">node), split, n, loss, yval, (yprob)</span><br><span class="line">      * denotes terminal node</span><br><span class="line"></span><br><span class="line">  <span class="number">1</span>) root <span class="number">2800</span> <span class="number">133</span> negative (<span class="number">0.003214286</span> <span class="number">0.044285714</span> <span class="number">0.952500000</span>)</span><br><span class="line">    <span class="number">2</span>) T4U&gt;=<span class="number">1.275</span> <span class="number">172</span>  <span class="number">82</span> increased binding protein (<span class="number">0.000000000</span> <span class="number">0.523255814</span> <span class="number">0.476744186</span>)</span><br><span class="line">      <span class="number">4</span>) T3&gt;=<span class="number">2.85</span> <span class="number">81</span>  <span class="number">13</span> increased binding protein (<span class="number">0.000000000</span> <span class="number">0.839506173</span> <span class="number">0.160493827</span>) *</span><br><span class="line">      <span class="number">5</span>) T3&lt; <span class="number">2.85</span> <span class="number">91</span>  <span class="number">22</span> negative (<span class="number">0.000000000</span> <span class="number">0.241758242</span> <span class="number">0.758241758</span>)</span><br><span class="line">       <span class="number">10</span>) T3.measured=f <span class="number">24</span>  <span class="number">11</span> increased binding protein (<span class="number">0.000000000</span> <span class="number">0.541666667</span> <span class="number">0.458333333</span>)</span><br><span class="line">         <span class="number">20</span>) TT4&lt; <span class="number">141.5</span> <span class="number">14</span>   <span class="number">3</span> increased binding protein (<span class="number">0.000000000</span> <span class="number">0.785714286</span> <span class="number">0.214285714</span>) *</span><br><span class="line">         <span class="number">21</span>) TT4&gt;=<span class="number">141.5</span> <span class="number">10</span>   <span class="number">2</span> negative (<span class="number">0.000000000</span> <span class="number">0.200000000</span> <span class="number">0.800000000</span>) *</span><br><span class="line">       <span class="number">11</span>) T3.measured=t <span class="number">67</span>   <span class="number">9</span> negative (<span class="number">0.000000000</span> <span class="number">0.134328358</span> <span class="number">0.865671642</span>)</span><br><span class="line">         <span class="number">22</span>) TT4&gt;=<span class="number">179.5</span> <span class="number">7</span>   <span class="number">2</span> increased binding protein (<span class="number">0.000000000</span> <span class="number">0.714285714</span> <span class="number">0.285714286</span>) *</span><br><span class="line">         <span class="number">23</span>) TT4&lt; <span class="number">179.5</span> <span class="number">60</span>   <span class="number">4</span> negative (<span class="number">0.000000000</span> <span class="number">0.066666667</span> <span class="number">0.933333333</span>) *</span><br><span class="line">    <span class="number">3</span>) T4U&lt; <span class="number">1.275</span> <span class="number">2628</span>  <span class="number">43</span> negative (<span class="number">0.003424658</span> <span class="number">0.012937595</span> <span class="number">0.983637747</span>)</span><br><span class="line">      <span class="number">6</span>) T3&gt;=<span class="number">2.85</span> <span class="number">150</span>  <span class="number">28</span> negative (<span class="number">0.000000000</span> <span class="number">0.186666667</span> <span class="number">0.813333333</span>)</span><br><span class="line">       <span class="number">12</span>) TSH&gt;=<span class="number">2.95</span> <span class="number">15</span>   <span class="number">5</span> increased binding protein (<span class="number">0.000000000</span> <span class="number">0.666666667</span> <span class="number">0.333333333</span>) *</span><br><span class="line">       <span class="number">13</span>) TSH&lt; <span class="number">2.95</span> <span class="number">135</span>  <span class="number">18</span> negative (<span class="number">0.000000000</span> <span class="number">0.133333333</span> <span class="number">0.866666667</span>)</span><br><span class="line">         <span class="number">26</span>) T4U&gt;=<span class="number">1.135</span> <span class="number">30</span>  <span class="number">11</span> negative (<span class="number">0.000000000</span> <span class="number">0.366666667</span> <span class="number">0.633333333</span>)</span><br><span class="line">           <span class="number">52</span>) TSH&lt; <span class="number">1.25</span> <span class="number">22</span>  <span class="number">11</span> increased binding protein (<span class="number">0.000000000</span> <span class="number">0.500000000</span> <span class="number">0.500000000</span>)</span><br><span class="line">            <span class="number">104</span>) TT4&gt;=<span class="number">143.5</span> <span class="number">10</span>   <span class="number">3</span> increased binding protein (<span class="number">0.000000000</span> <span class="number">0.700000000</span> <span class="number">0.300000000</span>) *</span><br><span class="line">            <span class="number">105</span>) TT4&lt; <span class="number">143.5</span> <span class="number">12</span>   <span class="number">4</span> negative (<span class="number">0.000000000</span> <span class="number">0.333333333</span> <span class="number">0.666666667</span>) *</span><br><span class="line">           <span class="number">53</span>) TSH&gt;=<span class="number">1.25</span> <span class="number">8</span>   <span class="number">0</span> negative (<span class="number">0.000000000</span> <span class="number">0.000000000</span> <span class="number">1.000000000</span>) *</span><br><span class="line">         <span class="number">27</span>) T4U&lt; <span class="number">1.135</span> <span class="number">105</span>   <span class="number">7</span> negative (<span class="number">0.000000000</span> <span class="number">0.066666667</span> <span class="number">0.933333333</span>) *</span><br><span class="line">      <span class="number">7</span>) T3&lt; <span class="number">2.85</span> <span class="number">2478</span>  <span class="number">15</span> negative (<span class="number">0.003631961</span> <span class="number">0.002421308</span> <span class="number">0.993946731</span>)</span><br><span class="line">       <span class="number">14</span>) T4U&lt; <span class="number">0.595</span> <span class="number">24</span>   <span class="number">8</span> negative (<span class="number">0.333333333</span> <span class="number">0.000000000</span> <span class="number">0.666666667</span>)</span><br><span class="line">         <span class="number">28</span>) TT4&lt; <span class="number">60</span> <span class="number">9</span>   <span class="number">2</span> decreased binding protein (<span class="number">0.777777778</span> <span class="number">0.000000000</span> <span class="number">0.222222222</span>) *</span><br><span class="line">         <span class="number">29</span>) TT4&gt;=<span class="number">60</span> <span class="number">15</span>   <span class="number">1</span> negative (<span class="number">0.066666667</span> <span class="number">0.000000000</span> <span class="number">0.933333333</span>) *</span><br><span class="line">       <span class="number">15</span>) T4U&gt;=<span class="number">0.595</span> <span class="number">2454</span>   <span class="number">7</span> negative (<span class="number">0.000407498</span> <span class="number">0.002444988</span> <span class="number">0.997147514</span>)</span><br><span class="line">         <span class="number">30</span>) TT4&gt;=<span class="number">159.5</span> <span class="number">92</span>   <span class="number">6</span> negative (<span class="number">0.000000000</span> <span class="number">0.065217391</span> <span class="number">0.934782609</span>)</span><br><span class="line">           <span class="number">60</span>) FTI&lt; <span class="number">141</span> <span class="number">8</span>   <span class="number">2</span> increased binding protein (<span class="number">0.000000000</span> <span class="number">0.750000000</span> <span class="number">0.250000000</span>) *</span><br><span class="line">           <span class="number">61</span>) FTI&gt;=<span class="number">141</span> <span class="number">84</span>   <span class="number">0</span> negative (<span class="number">0.000000000</span> <span class="number">0.000000000</span> <span class="number">1.000000000</span>) *</span><br><span class="line">         <span class="number">31</span>) TT4&lt; <span class="number">159.5</span> <span class="number">2362</span>   <span class="number">1</span> negative (<span class="number">0.000423370</span> <span class="number">0.000000000</span> <span class="number">0.999576630</span>) *</span><br></pre></td></tr></table></figure></p>
<p>生成图片<br><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plot(rt)</span><br><span class="line">text(rt)</span><br></pre></td></tr></table></figure></p>
<img src="/2014/12/24/决策树ID3-C4-5详解和python实现与R语言实现比较/20141225204838608.png" alt="20141225204838608.png" title="">
<p>R已经自动悲观剪枝了，这个模型已经非常适合了，如果非要剪枝的话：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">printcp(rt)</span><br><span class="line">rt2&lt;-prune(rt,cp=<span class="number">0.08</span>)</span><br></pre></td></tr></table></figure>
<p>预测一下：<br><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict(rt,testData)</span><br></pre></td></tr></table></figure></p>
<p>一大堆数据还看不出什么，生成数据评价一呗下<br><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(rt &lt;- mean(abs(rt.predictions-as.numeric(testData[,<span class="string">'classes'</span>]))))</span><br><span class="line"><span class="comment">#MAE 绝对平均误差</span></span><br><span class="line">(rt &lt;- mean((rt.predictions-as.numeric(testData[,<span class="string">'classes'</span>]))^<span class="number">2</span>))</span><br><span class="line"><span class="comment">#MSE 均方误差</span></span><br><span class="line">(rt &lt;- mean((rt.predictions-as.numeric(testData[,<span class="string">'classes'</span>]))^<span class="number">2</span>)/</span><br><span class="line">   mean((mean(as.numeric(testData[,<span class="string">'classes'</span>]))-as.numeric(testData[,<span class="string">'classes'</span>]))^<span class="number">2</span>))</span><br></pre></td></tr></table></figure></p>
<p>结果:<br><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(rt &lt;- mean(abs(rt.predictions-as.numeric(testData[,<span class="string">'classes'</span>]))))</span><br><span class="line"><span class="comment">#MAE 绝对平均误差</span></span><br><span class="line">(rt &lt;- mean((rt.predictions-as.numeric(testData[,<span class="string">'classes'</span>]))^<span class="number">2</span>))</span><br><span class="line"><span class="comment">#MSE 均方误差</span></span><br><span class="line">(rt &lt;- mean((rt.predictions-as.numeric(testData[,<span class="string">'classes'</span>]))^<span class="number">2</span>)/</span><br><span class="line">mean((mean(as.numeric(testData[,<span class="string">'classes'</span>]))-as.numeric(testData[,<span class="string">'classes'</span>]))^<span class="number">2</span>))</span><br></pre></td></tr></table></figure></p>
<p>结果还是理想的<br>最后，R实在是很强大-。-</p>
<p>参考文章：</p>
<p><a href="http://www.cnblogs.com/wentingtu/archive/2012/03/24/2416235.html" target="_blank" rel="external">http://www.cnblogs.com/wentingtu/archive/2012/03/24/2416235.html</a><br><a href="http://shiyanjun.cn/archives/428.html" target="_blank" rel="external">http://shiyanjun.cn/archives/428.html</a><br><a href="http://blog.sciencenet.cn/blog-629275-535565.html" target="_blank" rel="external">http://blog.sciencenet.cn/blog-629275-535565.html</a>（这个也是是转的，不过我找不到原作-。-）<br>《机器学习实战》</p>
  
	</div>
		<footer class="article-footer clearfix">

  <div class="article-tags">
  
  <span></span> <a href="/tags/c4-5/">c4.5</a><a href="/tags/id3/">id3</a><a href="/tags/python机器学习/">python机器学习</a><a href="/tags/机器学习/">机器学习</a><a href="/tags/预测/">预测</a>
  </div>




<div class="article-share" id="share">

  <div data-url="http://tyan.io/2014/12/24/决策树ID3-C4-5详解和python实现与R语言实现比较/" data-title="决策树ID3;C4.5详解和python实现与R语言实现比较 | Tyan | AbyChan | Blog" data-tsina="null" class="share clearfix">
  </div>

</div>
</footer>   	       
	</article>
	
<nav class="article-nav clearfix">
 
 <div class="prev" >
 <a href="/2014/12/28/Linux下uWSGI-NGINX部署Django-AngularJs/" title="Linux下uWSGI+NGINX部署Django+AngularJs">
  <strong>PREVIOUS:</strong><br/>
  <span>
  Linux下uWSGI+NGINX部署Django+AngularJs</span>
</a>
</div>


<div class="next">
<a href="/2014/12/20/R语言中RMySQL的简单应用/"  title="R语言中RMySQL的简单应用">
 <strong>NEXT:</strong><br/> 
 <span>R语言中RMySQL的简单应用
</span>
</a>
</div>

</nav>

	
</div>  
      <div class="openaside"><a class="navbutton" href="#" title="顯示側邊欄"></a></div>

  <div id="toc" class="toc-aside">
  <strong class="toc-title">文章目錄</strong>
  <ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-_信息论里的熵"><span class="toc-number">1.</span> <span class="toc-text">1. 信息论里的熵</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-_分类系统里的熵"><span class="toc-number">2.</span> <span class="toc-text">2. 分类系统里的熵</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-_信息增益和熵的关系"><span class="toc-number">3.</span> <span class="toc-text">3. 信息增益和熵的关系</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#一个例子："><span class="toc-number"></span> <span class="toc-text">一个例子：</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#用决策树来预测："><span class="toc-number"></span> <span class="toc-text">用决策树来预测：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#用熵来计算信息增益:"><span class="toc-number"></span> <span class="toc-text">用熵来计算信息增益:</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1_计算分类系统熵"><span class="toc-number">1.</span> <span class="toc-text">1 计算分类系统熵</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2_分别以Wind、Humidity、Outlook和Temperature作为根节点，计算其信息增益"><span class="toc-number">2.</span> <span class="toc-text">2 分别以Wind、Humidity、Outlook和Temperature作为根节点，计算其信息增益</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#C-45的算法"><span class="toc-number">3.</span> <span class="toc-text">C.45的算法</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#From_Garavan_Institute"><span class="toc-number"></span> <span class="toc-text">From Garavan Institute</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Documentation:_as_given_by_Ross_Quinlan"><span class="toc-number"></span> <span class="toc-text">Documentation: as given by Ross Quinlan</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6_databases_from_the_Garavan_Institute_in_Sydney,_Australia"><span class="toc-number"></span> <span class="toc-text">6 databases from the Garavan Institute in Sydney, Australia</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Approximately_the_following_for_each_database:"><span class="toc-number"></span> <span class="toc-text">Approximately the following for each database:</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#用R实现一下"><span class="toc-number"></span> <span class="toc-text">用R实现一下</span></a>
  </div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="隱藏側邊欄"></a></div>
<aside class="clearfix">

  

  
<div class="tagslist">
	<p class="asidetitle">標簽</p>
		<ul class="clearfix">
		
			<li><a href="/tags/R/" title="R">R<sup>1</sup></a></li>
		
			<li><a href="/tags/RMysql/" title="RMysql">RMysql<sup>1</sup></a></li>
		
			<li><a href="/tags/Web前端架构/" title="Web前端架构">Web前端架构<sup>1</sup></a></li>
		
			<li><a href="/tags/angular/" title="angular">angular<sup>1</sup></a></li>
		
			<li><a href="/tags/arm/" title="arm">arm<sup>1</sup></a></li>
		
			<li><a href="/tags/binary/" title="binary">binary<sup>1</sup></a></li>
		
			<li><a href="/tags/c4-5/" title="c4.5">c4.5<sup>1</sup></a></li>
		
			<li><a href="/tags/cb4/" title="cb4">cb4<sup>1</sup></a></li>
		
			<li><a href="/tags/chrome/" title="chrome">chrome<sup>1</sup></a></li>
		
			<li><a href="/tags/chroot/" title="chroot">chroot<sup>2</sup></a></li>
		
			<li><a href="/tags/chroot-jail/" title="chroot jail">chroot jail<sup>1</sup></a></li>
		
			<li><a href="/tags/css/" title="css">css<sup>1</sup></a></li>
		
			<li><a href="/tags/css3/" title="css3">css3<sup>1</sup></a></li>
		
			<li><a href="/tags/cubieboard/" title="cubieboard">cubieboard<sup>1</sup></a></li>
		
			<li><a href="/tags/database/" title="database">database<sup>3</sup></a></li>
		
			<li><a href="/tags/dhcp/" title="dhcp">dhcp<sup>1</sup></a></li>
		
			<li><a href="/tags/django/" title="django">django<sup>2</sup></a></li>
		
			<li><a href="/tags/emacs/" title="emacs">emacs<sup>1</sup></a></li>
		
			<li><a href="/tags/emasc美化/" title="emasc美化">emasc美化<sup>1</sup></a></li>
		
			<li><a href="/tags/fcitx/" title="fcitx">fcitx<sup>1</sup></a></li>
		
		</ul>
</div>


  <div class="rsspart">
	<a href="null" target="_blank" title="rss">RSS 訂閱</a>
</div>

</aside>
</div>
    </div>
    <footer><div id="footer" >
	
	<div class="line">
		<span></span>
		<div class="author"></div>
	</div>
	
	
	<div class="social-font clearfix">
		
		
		
		
		
	</div>
		<p class="copyright">Powered by <a href="http://hexo.io" target="_blank" title="hexo">hexo</a> and Theme by <a href="https://github.com/A-limon/pacman" target="_blank" title="Pacman">Pacman</a> © 2015 
		
		<a href="http://tyan.io" target="_blank" title="Tyan">Tyan</a>
		
		</p>
</div>
</footer>
    <script src="/js/jquery-2.1.0.min.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else
    {
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
      
      $('#toc.toc-aside').css('display', 'none');
        
    }
  });
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      h  = $('article h2')
      ah = $('article h2'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  if(ah.length==0){
    t.css('display','none');
  }else{
    c.click(function(){
      ta.css('display', 'block').addClass('fadeIn');
    });
    o.click(function(){
      ta.css('display', 'none');
    });
    $(window).scroll(function(){
      ta.css("top",Math.max(140,320-$(this).scrollTop()));
    });
  };
});
</script>


<script type="text/javascript">
$(document).ready(function(){ 
  var $this = $('.share'),
      url = $this.attr('data-url'),
      encodedUrl = encodeURIComponent(url),
      title = $this.attr('data-title'),
      tsina = $this.attr('data-tsina');
  var html = [
  '<a href="#" class="overlay" id="qrcode"></a>',
  '<div class="qrcode clearfix"><span>扫描二维码分享到微信朋友圈</span><a class="qrclose" href="#share"></a><strong>Loading...Please wait</strong><img id="qrcode-pic" data-src="http://s.jiathis.com/qrcode.php?url=' + encodedUrl + '"/></div>',
  '<a href="#textlogo" class="article-back-to-top" title="Top"></a>',
  '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
  '<a href="#qrcode" class="article-share-qrcode" title="QRcode"></a>',
  '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
  '<a href="http://service.weibo.com/share/share.php?title='+title+'&url='+encodedUrl +'&ralateUid='+ tsina +'&searchPic=true&style=number' +'" class="article-share-weibo" target="_blank" title="Weibo"></a>',
  '<span title="Share to"></span>'
  ].join('');
  $this.append(html);
  $('.article-share-qrcode').click(function(){
    var imgSrc = $('#qrcode-pic').attr('data-src');
    $('#qrcode-pic').attr('src', imgSrc);
    $('#qrcode-pic').load(function(){
        $('.qrcode strong').text(' ');
    });
  });
});     
</script>






  </body>
</html>
